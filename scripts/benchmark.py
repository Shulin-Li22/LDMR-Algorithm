#!/usr/bin/env python3
"""
åŸºå‡†æµ‹è¯•è„šæœ¬
è¿è¡ŒLDMRä¸å…¶ä»–ç®—æ³•çš„æ€§èƒ½å¯¹æ¯”
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent
sys.path.insert(0, str(project_root / 'src'))

from topology.satellite_constellation import LEONetworkBuilder
from traffic.traffic_model import TrafficGenerator
from algorithms.baseline import BenchmarkManager
from config import load_scenario_config
from runner import setup_logger


def run_single_scenario_benchmark(scenario_name: str = 'testing'):
    """è¿è¡Œå•åœºæ™¯åŸºå‡†æµ‹è¯•"""
    print(f"ğŸ”¬ å¼€å§‹åŸºå‡†æµ‹è¯• - åœºæ™¯: {scenario_name}")
    print("=" * 60)
    
    try:
        # åŠ è½½åœºæ™¯é…ç½®
        config = load_scenario_config(scenario_name)
        logger = setup_logger(config.get('output', {}))
        
        # æ„å»ºç½‘ç»œæ‹“æ‰‘
        print("ğŸ”§ æ„å»ºç½‘ç»œæ‹“æ‰‘...")
        network_config = config['network']
        builder = LEONetworkBuilder(
            network_config['constellation'], 
            network_config['ground_stations']
        )
        topology = builder.build_network()
        
        network_stats = topology.get_statistics()
        print(f"   ç½‘ç»œ: {network_stats['total_nodes']}èŠ‚ç‚¹, {network_stats['total_links']}é“¾è·¯")
        
        # ç”Ÿæˆæµé‡éœ€æ±‚
        print("ğŸ“ˆ ç”Ÿæˆæµé‡éœ€æ±‚...")
        traffic_config = config['traffic']
        generator = TrafficGenerator()
        
        ground_stations = [
            node.id for node in topology.nodes.values()
            if node.type.value == 'ground_station'
        ]
        
        demands = generator.generate_traffic_demands(
            ground_station_ids=ground_stations,
            total_traffic=traffic_config['total_gbps'],
            duration=traffic_config['duration'],
            elephant_ratio=traffic_config['elephant_ratio']
        )
        
        print(f"   ç”Ÿæˆ {len(demands)} ä¸ªæµé‡éœ€æ±‚")
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        print("\nğŸš€ å¼€å§‹ç®—æ³•å¯¹æ¯”...")
        manager = BenchmarkManager()
        
        # è¿è¡Œæ‰€æœ‰ç®—æ³•
        results = manager.run_benchmark(topology, demands, ['LDMR', 'SPF', 'ECMP'])
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
        print(manager.generate_comparison_table(results))
        
        # ä¿å­˜ç»“æœ
        result_files = manager.save_results(results, config['output']['results_dir'])
        
        return results
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def run_multi_scenario_benchmark():
    """è¿è¡Œå¤šåœºæ™¯åŸºå‡†æµ‹è¯•"""
    scenarios = ['testing', 'light_load', 'heavy_load', 'performance']
    
    print("ğŸ”¬ å¤šåœºæ™¯åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    all_results = {}
    
    for scenario in scenarios:
        print(f"\nğŸ“‹ åœºæ™¯: {scenario}")
        print("-" * 40)
        
        try:
            results = run_single_scenario_benchmark(scenario)
            if results:
                all_results[scenario] = results
                
                # æ˜¾ç¤ºè¯¥åœºæ™¯çš„ç®€è¦ç»“æœ
                for algo_name, data in results.items():
                    if 'error' not in data:
                        metrics = data['metrics']
                        print(f"   {algo_name}: æˆåŠŸç‡={metrics.get('success_rate', 0):.1%}, "
                              f"å»¶è¿Ÿ={metrics.get('avg_path_delay', 0):.3f}ms")
                    else:
                        print(f"   {algo_name}: å¤±è´¥")
            
        except Exception as e:
            print(f"   âŒ åœºæ™¯ {scenario} å¤±è´¥: {e}")
    
    # ç”Ÿæˆç»¼åˆå¯¹æ¯”
    if all_results:
        print("\nğŸ“Š å¤šåœºæ™¯ç»¼åˆå¯¹æ¯”:")
        print("=" * 80)
        generate_multi_scenario_summary(all_results)
    
    return all_results


def generate_multi_scenario_summary(all_results: dict):
    """ç”Ÿæˆå¤šåœºæ™¯ç»¼åˆå¯¹æ¯”"""
    algorithms = ['LDMR', 'SPF', 'ECMP']
    scenarios = list(all_results.keys())
    
    print(f"{'åœºæ™¯':<12} {'ç®—æ³•':<8} {'æˆåŠŸç‡':<8} {'å¹³å‡å»¶è¿Ÿ(ms)':<12} {'æ‰§è¡Œæ—¶é—´(s)':<12}")
    print("-" * 60)
    
    for scenario in scenarios:
        results = all_results[scenario]
        for algo in algorithms:
            if algo in results and 'error' not in results[algo]:
                metrics = results[algo]['metrics']
                success_rate = metrics.get('success_rate', 0)
                avg_delay = metrics.get('avg_path_delay', 0)
                exec_time = metrics.get('execution_time', 0)
                
                print(f"{scenario:<12} {algo:<8} {success_rate:<8.1%} "
                      f"{avg_delay:<12.3f} {exec_time:<12.2f}")
            else:
                print(f"{scenario:<12} {algo:<8} {'ERROR':<8} {'N/A':<12} {'N/A':<12}")


def run_algorithm_comparison():
    """è¿è¡Œç®—æ³•è¯¦ç»†å¯¹æ¯”"""
    print("ğŸ”¬ ç®—æ³•è¯¦ç»†å¯¹æ¯”åˆ†æ")
    print("=" * 60)
    
    # ä½¿ç”¨performanceåœºæ™¯è¿›è¡Œè¯¦ç»†å¯¹æ¯”
    config = load_scenario_config('performance')
    
    # æ„å»ºç½‘ç»œ
    network_config = config['network']
    builder = LEONetworkBuilder(
        network_config['constellation'], 
        network_config['ground_stations']
    )
    topology = builder.build_network()
    
    # ç”Ÿæˆæµé‡
    traffic_config = config['traffic']
    generator = TrafficGenerator()
    ground_stations = [
        node.id for node in topology.nodes.values()
        if node.type.value == 'ground_station'
    ]
    
    demands = generator.generate_traffic_demands(
        ground_station_ids=ground_stations,
        total_traffic=traffic_config['total_gbps'],
        duration=traffic_config['duration']
    )
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    manager = BenchmarkManager()
    results = manager.run_benchmark(topology, demands)
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print("\nğŸ“‹ è¯¦ç»†æ€§èƒ½æŠ¥å‘Š:")
    print(manager.generate_detailed_report(results))
    
    return results


def interactive_benchmark():
    """äº¤äº’å¼åŸºå‡†æµ‹è¯•"""
    while True:
        print("\n" + "="*50)
        print("ğŸ”¬ åŸºå‡†æµ‹è¯•å·¥å…·")
        print("="*50)
        print("1. ğŸ§ª å•åœºæ™¯åŸºå‡†æµ‹è¯•")
        print("2. ğŸ“Š å¤šåœºæ™¯åŸºå‡†æµ‹è¯•")
        print("3. ğŸ“‹ ç®—æ³•è¯¦ç»†å¯¹æ¯”")
        print("4. ğŸ”„ è‡ªå®šä¹‰æµ‹è¯•")
        print("5. âŒ é€€å‡º")
        print("="*50)
        
        choice = input("è¯·é€‰æ‹©æ“ä½œ (1-5): ").strip()
        
        if choice == '1':
            scenarios = ['testing', 'light_load', 'heavy_load', 'performance']
            print("\nå¯ç”¨åœºæ™¯:")
            for i, scenario in enumerate(scenarios, 1):
                print(f"  {i}. {scenario}")
            
            try:
                idx = int(input("è¯·é€‰æ‹©åœºæ™¯ (1-4): ")) - 1
                if 0 <= idx < len(scenarios):
                    run_single_scenario_benchmark(scenarios[idx])
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                
        elif choice == '2':
            run_multi_scenario_benchmark()
            
        elif choice == '3':
            run_algorithm_comparison()
            
        elif choice == '4':
            print("ğŸ“ è‡ªå®šä¹‰æµ‹è¯•é…ç½®:")
            scenario = input("åœºæ™¯åç§° [performance]: ").strip() or 'performance'
            run_single_scenario_benchmark(scenario)
            
        elif choice == '5':
            print("ğŸ‘‹ é€€å‡º")
            break
            
        else:
            print("âŒ æ— æ•ˆé€‰é¡¹")


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == 'single':
            scenario = sys.argv[2] if len(sys.argv) > 2 else 'testing'
            run_single_scenario_benchmark(scenario)
            
        elif command == 'multi':
            run_multi_scenario_benchmark()
            
        elif command == 'compare':
            run_algorithm_comparison()
            
        elif command == 'interactive':
            interactive_benchmark()
            
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
            print("æ”¯æŒçš„å‘½ä»¤:")
            print("  single [scenario]  - å•åœºæ™¯åŸºå‡†æµ‹è¯•")
            print("  multi             - å¤šåœºæ™¯åŸºå‡†æµ‹è¯•") 
            print("  compare           - ç®—æ³•è¯¦ç»†å¯¹æ¯”")
            print("  interactive       - äº¤äº’å¼æ¨¡å¼")
    else:
        interactive_benchmark()


if __name__ == "__main__":
    main()
