#!/usr/bin/env python3
"""
å¢å¼ºçš„åŸºå‡†æµ‹è¯•ç»“æœåˆ†æå·¥å…·
ä½¿ç”¨æ–¹æ³•: python scripts/enhanced_analysis.py results/benchmark_results_XXXXXX.json
"""

import json
import sys
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def load_benchmark_results(filepath):
    """åŠ è½½åŸºå‡†æµ‹è¯•ç»“æœ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def calculate_jain_fairness_index(values):
    """è®¡ç®—Jainå…¬å¹³æ€§æŒ‡æ•°"""
    if not values:
        return 0.0
    
    n = len(values)
    sum_x = sum(values)
    sum_x_squared = sum(x**2 for x in values)
    
    if sum_x_squared == 0:
        return 1.0
    
    return (sum_x ** 2) / (n * sum_x_squared)

def analyze_results_deeply(results):
    """æ·±å…¥åˆ†æåŸºå‡†æµ‹è¯•ç»“æœ"""
    
    print("ğŸ” æ·±å…¥åˆ†æåŸºå‡†æµ‹è¯•ç»“æœ")
    print("=" * 60)
    
    algorithms = list(results.keys())
    
    # 1. åŸºç¡€æŒ‡æ ‡å¯¹æ¯”
    print("\nğŸ“Š åŸºç¡€æŒ‡æ ‡å¯¹æ¯”:")
    print(f"{'æŒ‡æ ‡':<15} {'LDMR':<10} {'SPF':<10} {'ECMP':<10}")
    print("-" * 50)
    
    metrics = ['success_rate', 'avg_path_delay', 'avg_paths_per_demand', 'execution_time']
    metric_names = ['æˆåŠŸç‡', 'å¹³å‡å»¶è¿Ÿ(ms)', 'å¹³å‡è·¯å¾„æ•°', 'æ‰§è¡Œæ—¶é—´(s)']
    
    for metric, name in zip(metrics, metric_names):
        values = []
        for algo in algorithms:
            if metric == 'success_rate':
                val = f"{results[algo]['metrics'][metric]:.1%}"
            elif metric == 'avg_path_delay':
                val = f"{results[algo]['metrics'][metric]*1000:.3f}"
            else:
                val = f"{results[algo]['metrics'][metric]:.2f}"
            values.append(val)
        
        print(f"{name:<15} {values[0]:<10} {values[1]:<10} {values[2]:<10}")
    
    # 2. è·¯å¾„è´¨é‡åˆ†æ
    print("\nğŸ›£ï¸ è·¯å¾„è´¨é‡åˆ†æ:")
    print(f"{'ç®—æ³•':<8} {'æ€»è·¯å¾„':<8} {'å¹³å‡é•¿åº¦':<10} {'æœ€çŸ­è·¯å¾„':<10} {'æœ€é•¿è·¯å¾„':<10}")
    print("-" * 50)
    
    for algo in algorithms:
        metrics = results[algo]['metrics']
        total_paths = metrics.get('total_paths', 0)
        avg_length = metrics.get('avg_path_length', 0)
        min_length = metrics.get('min_path_length', 0)
        max_length = metrics.get('max_path_length', 0)
        
        print(f"{algo:<8} {total_paths:<8} {avg_length:<10.1f} {min_length:<10} {max_length:<10}")
    
    # 3. å»¶è¿Ÿåˆ†å¸ƒåˆ†æ
    print("\nâ±ï¸ å»¶è¿Ÿåˆ†å¸ƒåˆ†æ:")
    print(f"{'ç®—æ³•':<8} {'æœ€å°å»¶è¿Ÿ':<10} {'å¹³å‡å»¶è¿Ÿ':<10} {'æœ€å¤§å»¶è¿Ÿ':<10} {'å»¶è¿ŸèŒƒå›´':<10}")
    print("-" * 55)
    
    for algo in algorithms:
        metrics = results[algo]['metrics']
        min_delay = metrics.get('min_path_delay', 0) * 1000  # è½¬æ¢ä¸ºms
        avg_delay = metrics.get('avg_path_delay', 0) * 1000
        max_delay = metrics.get('max_path_delay', 0) * 1000
        delay_range = max_delay - min_delay
        
        print(f"{algo:<8} {min_delay:<10.2f} {avg_delay:<10.2f} {max_delay:<10.2f} {delay_range:<10.2f}")
    
    # 4. æ•ˆç‡åˆ†æ
    print("\nâš¡ æ•ˆç‡åˆ†æ:")
    print(f"{'ç®—æ³•':<8} {'æ¯éœ€æ±‚è€—æ—¶(ms)':<15} {'æ¯è·¯å¾„è€—æ—¶(ms)':<15} {'ç›¸å¯¹æ•ˆç‡':<10}")
    print("-" * 55)
    
    ldmr_time = results['LDMR']['metrics']['execution_time']
    
    for algo in algorithms:
        metrics = results[algo]['metrics']
        exec_time = metrics['execution_time']
        total_demands = metrics['total_demands']
        total_paths = metrics['total_paths']
        
        time_per_demand = (exec_time / total_demands) * 1000
        time_per_path = (exec_time / total_paths) * 1000
        relative_efficiency = ldmr_time / exec_time
        
        print(f"{algo:<8} {time_per_demand:<15.3f} {time_per_path:<15.3f} {relative_efficiency:<10.2f}")

def plot_comparison_charts(results, output_dir="results/figures"):
    """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    algorithms = list(results.keys())
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå­å›¾
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('LDMR vs Baseline Algorithms Performance Comparison', fontsize=16, fontweight='bold')
    
    # 1. å»¶è¿Ÿå¯¹æ¯”
    delays = [results[algo]['metrics']['avg_path_delay'] * 1000 for algo in algorithms]
    bars1 = ax1.bar(algorithms, delays, color=colors)
    ax1.set_title('Average Path Delay Comparison')
    ax1.set_ylabel('Delay (ms)')
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, delay in zip(bars1, delays):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                f'{delay:.3f}', ha='center', va='bottom')
    
    # 2. è·¯å¾„æ•°é‡å¯¹æ¯”
    path_counts = [results[algo]['metrics']['avg_paths_per_demand'] for algo in algorithms]
    bars2 = ax2.bar(algorithms, path_counts, color=colors)
    ax2.set_title('Average Paths per Demand')
    ax2.set_ylabel('Paths per Demand')
    ax2.grid(True, alpha=0.3)
    
    for bar, count in zip(bars2, path_counts):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,
                f'{count:.1f}', ha='center', va='bottom')
    
    # 3. æ‰§è¡Œæ—¶é—´å¯¹æ¯”
    exec_times = [results[algo]['metrics']['execution_time'] for algo in algorithms]
    bars3 = ax3.bar(algorithms, exec_times, color=colors)
    ax3.set_title('Execution Time Comparison')
    ax3.set_ylabel('Time (seconds)')
    ax3.grid(True, alpha=0.3)
    
    for bar, time in zip(bars3, exec_times):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                f'{time:.2f}', ha='center', va='bottom')
    
    # 4. è·¯å¾„é•¿åº¦åˆ†å¸ƒå¯¹æ¯”
    path_lengths = [results[algo]['metrics']['avg_path_length'] for algo in algorithms]
    bars4 = ax4.bar(algorithms, path_lengths, color=colors)
    ax4.set_title('Average Path Length Comparison')
    ax4.set_ylabel('Hops')
    ax4.grid(True, alpha=0.3)
    
    for bar, length in zip(bars4, path_lengths):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,
                f'{length:.1f}', ha='center', va='bottom')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    chart_file = f"{output_dir}/algorithm_comparison.png"
    plt.savefig(chart_file, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {chart_file}")
    
    return chart_file

def generate_insights(results):
    """ç”Ÿæˆåˆ†ææ´å¯Ÿ"""
    
    print("\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
    print("=" * 40)
    
    ldmr_metrics = results['LDMR']['metrics']
    spf_metrics = results['SPF']['metrics']
    ecmp_metrics = results['ECMP']['metrics']
    
    # 1. å»¶è¿Ÿåˆ†æ
    ldmr_delay = ldmr_metrics['avg_path_delay'] * 1000
    spf_delay = spf_metrics['avg_path_delay'] * 1000
    ecmp_delay = ecmp_metrics['avg_path_delay'] * 1000
    
    delay_overhead = ((ldmr_delay - spf_delay) / spf_delay) * 100
    
    print(f"1. ğŸ“ˆ LDMRå»¶è¿Ÿæ¯”SPFé«˜ {delay_overhead:.1f}%")
    print(f"   è¿™æ˜¯é“¾è·¯ä¸ç›¸äº¤çº¦æŸçš„ä»£ä»·")
    
    # 2. å¤šè·¯å¾„ä¼˜åŠ¿
    ldmr_paths = ldmr_metrics['avg_paths_per_demand']
    spf_paths = spf_metrics['avg_paths_per_demand']
    
    print(f"\n2. ğŸ›£ï¸ LDMRæä¾› {ldmr_paths:.0f} æ¡é“¾è·¯ä¸ç›¸äº¤è·¯å¾„")
    print(f"   ç›¸æ¯”SPFçš„å•è·¯å¾„ï¼Œæä¾›æ›´å¥½çš„å®¹é”™æ€§")
    
    # 3. è®¡ç®—å¤æ‚åº¦
    ldmr_time = ldmr_metrics['execution_time']
    spf_time = spf_metrics['execution_time']
    ecmp_time = ecmp_metrics['execution_time']
    
    print(f"\n3. âš¡ è®¡ç®—å¤æ‚åº¦:")
    print(f"   SPF: {spf_time:.2f}s (åŸºå‡†)")
    print(f"   LDMR: {ldmr_time:.2f}s ({ldmr_time/spf_time:.1f}x)")
    print(f"   ECMP: {ecmp_time:.2f}s ({ecmp_time/spf_time:.1f}x)")

def suggest_improvements():
    """å»ºè®®æ”¹è¿›æ–¹å‘"""
    
    print("\nğŸš€ æ”¹è¿›å»ºè®®:")
    print("=" * 40)
    
    print("1. ğŸ“Š å¢åŠ å‹åŠ›æµ‹è¯•:")
    print("   â€¢ é«˜æµé‡è´Ÿè½½æµ‹è¯• (15-25 Gbps)")
    print("   â€¢ ç½‘ç»œæ‹¥å¡åœºæ™¯æ¨¡æ‹Ÿ")
    print("   â€¢ ç“¶é¢ˆé“¾è·¯å¸¦å®½é™åˆ¶")
    
    print("\n2. ğŸ”§ å¢åŠ æ•…éšœæµ‹è¯•:")
    print("   â€¢ é“¾è·¯æ•…éšœæ¢å¤æµ‹è¯•")
    print("   â€¢ èŠ‚ç‚¹æ•…éšœå®¹é”™æµ‹è¯•")
    print("   â€¢ åŠ¨æ€æ‹“æ‰‘å˜åŒ–æµ‹è¯•")
    
    print("\n3. ğŸ“ˆ ä¼˜åŒ–è¯„ä¼°æŒ‡æ ‡:")
    print("   â€¢ è´Ÿè½½å‡è¡¡æŒ‡æ•° (Jain's Fairness)")
    print("   â€¢ ç½‘ç»œåˆ©ç”¨ç‡")
    print("   â€¢ ç«¯åˆ°ç«¯ååé‡")

def main():
    """ä¸»å‡½æ•°"""
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python scripts/enhanced_analysis.py <results_file.json>")
        print("ç¤ºä¾‹: python scripts/enhanced_analysis.py results/benchmark_results_20250717_233946.json")
        return
    
    results_file = sys.argv[1]
    
    try:
        # åŠ è½½ç»“æœ
        print(f"ğŸ“‚ åŠ è½½ç»“æœæ–‡ä»¶: {results_file}")
        results = load_benchmark_results(results_file)
        
        # æ·±å…¥åˆ†æ
        analyze_results_deeply(results)
        
        # ç”Ÿæˆå›¾è¡¨
        chart_file = plot_comparison_charts(results)
        
        # ç”Ÿæˆæ´å¯Ÿ
        generate_insights(results)
        
        # å»ºè®®æ”¹è¿›
        suggest_improvements()
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼å›¾è¡¨å·²ä¿å­˜: {chart_file}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
