"""
åŸºå‡†æµ‹è¯•ç®¡ç†å™¨
ç®¡ç†å¤šç®—æ³•å¯¹æ¯”æµ‹è¯•
"""

import time
import numpy as np
from typing import List, Dict, Any, Tuple
from pathlib import Path
import sys

# æ·»åŠ è·¯å¾„
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root / 'src'))

from topology.topology_base import NetworkTopology
from traffic.traffic_model import TrafficDemand
from algorithms.ldmr_algorithms import LDMRAlgorithm, LDMRConfig
from algorithms.baseline.spf_algorithm import SPFAlgorithm
from algorithms.baseline.ecmp_algorithm import ECMPAlgorithm
from algorithms.baseline.baseline_interface import AlgorithmResult


class BenchmarkManager:
    """åŸºå‡†æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.algorithms = {}
        self.results = {}
        
        # æ³¨å†Œé»˜è®¤ç®—æ³•
        self._register_default_algorithms()
    
    def _register_default_algorithms(self):
        """æ³¨å†Œé»˜è®¤ç®—æ³•"""
        # LDMRç®—æ³•
        ldmr_config = LDMRConfig(
            K=2, r1=1, r2=10, r3=50, Ne_th=2, enable_statistics=True
        )
        self.algorithms['LDMR'] = LDMRAlgorithm(ldmr_config)
        
        # SPFç®—æ³•
        self.algorithms['SPF'] = SPFAlgorithm({'weight_type': 'delay'})
        
        # ECMPç®—æ³•
        self.algorithms['ECMP'] = ECMPAlgorithm({'max_paths': 4, 'tolerance': 0.1})
    
    def register_algorithm(self, name: str, algorithm):
        """æ³¨å†Œç®—æ³•"""
        self.algorithms[name] = algorithm
    
    def _convert_ldmr_to_baseline_results(self, ldmr_results: List, algorithm_name: str) -> List[AlgorithmResult]:
        """å°†LDMRç»“æœè½¬æ¢ä¸ºåŸºå‡†ç®—æ³•ç»“æœæ ¼å¼"""
        baseline_results = []
        
        for result in ldmr_results:
            # åˆ›å»ºAlgorithmResult
            algo_result = AlgorithmResult(
                algorithm_name=algorithm_name,
                demand=result.demand,
                paths=result.paths,
                success=result.success,
                computation_time=result.computation_time,
                metadata={
                    'total_delay': result.total_delay,
                    'min_delay': result.min_delay,
                    'total_hops': result.total_hops
                }
            )
            baseline_results.append(algo_result)
        
        return baseline_results
    
    def run_single_algorithm(self, algorithm_name: str, topology: NetworkTopology, 
                           traffic_demands: List[TrafficDemand]) -> List[AlgorithmResult]:
        """è¿è¡Œå•ä¸ªç®—æ³•"""
        if algorithm_name not in self.algorithms:
            raise ValueError(f"ç®—æ³• {algorithm_name} æœªæ³¨å†Œ")
        
        algorithm = self.algorithms[algorithm_name]
        
        print(f"ğŸ”„ è¿è¡Œ {algorithm_name} ç®—æ³•...")
        start_time = time.time()
        
        if algorithm_name == 'LDMR':
            # LDMRç®—æ³•ç‰¹æ®Šå¤„ç†
            ldmr_results = algorithm.run_ldmr_algorithm(topology, traffic_demands)
            results = self._convert_ldmr_to_baseline_results(ldmr_results, algorithm_name)
        else:
            # åŸºå‡†ç®—æ³•
            results = algorithm.run_algorithm(topology, traffic_demands)
        
        execution_time = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        metrics = self._calculate_metrics(results, algorithm_name, execution_time)
        
        print(f"âœ… {algorithm_name} å®Œæˆ: æˆåŠŸç‡={metrics['success_rate']:.2%}, "
              f"å»¶è¿Ÿ={metrics.get('avg_path_delay', 0):.3f}ms")
        
        return results, metrics
    
    def run_benchmark(self, topology: NetworkTopology, 
                     traffic_demands: List[TrafficDemand],
                     algorithms: List[str] = None) -> Dict[str, Any]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        if algorithms is None:
            algorithms = list(self.algorithms.keys())
        
        print(f"ğŸš€ å¼€å§‹åŸºå‡†æµ‹è¯•")
        print(f"   ç®—æ³•: {', '.join(algorithms)}")
        print(f"   æµé‡éœ€æ±‚: {len(traffic_demands)} ä¸ª")
        
        benchmark_results = {}
        
        for algorithm_name in algorithms:
            try:
                results, metrics = self.run_single_algorithm(
                    algorithm_name, topology, traffic_demands)
                
                benchmark_results[algorithm_name] = {
                    'results': results,
                    'metrics': metrics
                }
                
            except Exception as e:
                print(f"âŒ {algorithm_name} æ‰§è¡Œå¤±è´¥: {e}")
                benchmark_results[algorithm_name] = {
                    'results': [],
                    'metrics': {},
                    'error': str(e)
                }
        
        print(f"âœ… åŸºå‡†æµ‹è¯•å®Œæˆ")
        return benchmark_results
    
    def _calculate_metrics(self, results: List[AlgorithmResult], 
                          algorithm_name: str, execution_time: float) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        if not results:
            return {'algorithm_name': algorithm_name, 'execution_time': execution_time}
        
        successful_results = [r for r in results if r.success]
        
        metrics = {
            'algorithm_name': algorithm_name,
            'total_demands': len(results),
            'successful_demands': len(successful_results),
            'failed_demands': len(results) - len(successful_results),
            'success_rate': len(successful_results) / len(results),
            'execution_time': execution_time
        }
        
        if successful_results:
            # è·¯å¾„ç»Ÿè®¡
            all_paths = []
            for result in successful_results:
                all_paths.extend(result.paths)
            
            if all_paths:
                metrics.update({
                    'total_paths': len(all_paths),
                    'avg_paths_per_demand': len(all_paths) / len(successful_results),
                    'avg_path_length': np.mean([p.length for p in all_paths]),
                    'min_path_length': min(p.length for p in all_paths),
                    'max_path_length': max(p.length for p in all_paths),
                    'avg_path_delay': np.mean([p.total_delay for p in all_paths]),
                    'min_path_delay': min(p.total_delay for p in all_paths),
                    'max_path_delay': max(p.total_delay for p in all_paths),
                })
            
            # è®¡ç®—æ—¶é—´ç»Ÿè®¡
            computation_times = [r.computation_time for r in results if r.computation_time > 0]
            if computation_times:
                metrics.update({
                    'avg_computation_time': np.mean(computation_times),
                    'total_computation_time': sum(computation_times),
                    'max_computation_time': max(computation_times),
                })
        
        return metrics
    
    def generate_comparison_table(self, benchmark_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼"""
        if not benchmark_results:
            return "æ²¡æœ‰åŸºå‡†æµ‹è¯•ç»“æœ"
        
        # è¡¨å¤´
        table = "ç®—æ³•æ€§èƒ½å¯¹æ¯”è¡¨\n"
        table += "=" * 80 + "\n"
        table += f"{'ç®—æ³•':<10} {'æˆåŠŸç‡':<8} {'å¹³å‡å»¶è¿Ÿ(ms)':<12} {'å¹³å‡è·¯å¾„æ•°':<10} {'æ‰§è¡Œæ—¶é—´(s)':<12}\n"
        table += "-" * 80 + "\n"
        
        # æ•°æ®è¡Œ
        for algorithm_name, data in benchmark_results.items():
            if 'error' in data:
                table += f"{algorithm_name:<10} {'ERROR':<8} {'N/A':<12} {'N/A':<10} {'N/A':<12}\n"
                continue
            
            metrics = data['metrics']
            success_rate = metrics.get('success_rate', 0)
            avg_delay = metrics.get('avg_path_delay', 0)
            avg_paths = metrics.get('avg_paths_per_demand', 0)
            exec_time = metrics.get('execution_time', 0)
            
            table += f"{algorithm_name:<10} {success_rate:<8.2%} {avg_delay:<12.3f} "
            table += f"{avg_paths:<10.1f} {exec_time:<12.2f}\n"
        
        table += "=" * 80
        return table
    
    def generate_detailed_report(self, benchmark_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report = "åŸºå‡†æµ‹è¯•è¯¦ç»†æŠ¥å‘Š\n"
        report += "=" * 60 + "\n\n"
        
        for algorithm_name, data in benchmark_results.items():
            report += f"ç®—æ³•: {algorithm_name}\n"
            report += "-" * 30 + "\n"
            
            if 'error' in data:
                report += f"âŒ æ‰§è¡Œå¤±è´¥: {data['error']}\n\n"
                continue
            
            metrics = data['metrics']
            
            # åŸºç¡€æŒ‡æ ‡
            report += f"æˆåŠŸç‡: {metrics.get('success_rate', 0):.2%}\n"
            report += f"æˆåŠŸéœ€æ±‚æ•°: {metrics.get('successful_demands', 0)}\n"
            report += f"å¤±è´¥éœ€æ±‚æ•°: {metrics.get('failed_demands', 0)}\n"
            report += f"æ‰§è¡Œæ—¶é—´: {metrics.get('execution_time', 0):.2f}s\n"
            
            # è·¯å¾„æŒ‡æ ‡
            if metrics.get('total_paths', 0) > 0:
                report += f"æ€»è·¯å¾„æ•°: {metrics.get('total_paths', 0)}\n"
                report += f"å¹³å‡è·¯å¾„æ•°/éœ€æ±‚: {metrics.get('avg_paths_per_demand', 0):.1f}\n"
                report += f"å¹³å‡è·¯å¾„é•¿åº¦: {metrics.get('avg_path_length', 0):.1f} è·³\n"
                report += f"å¹³å‡è·¯å¾„å»¶è¿Ÿ: {metrics.get('avg_path_delay', 0):.3f}ms\n"
                report += f"æœ€å°è·¯å¾„å»¶è¿Ÿ: {metrics.get('min_path_delay', 0):.3f}ms\n"
                report += f"æœ€å¤§è·¯å¾„å»¶è¿Ÿ: {metrics.get('max_path_delay', 0):.3f}ms\n"
            
            # è®¡ç®—æ—¶é—´æŒ‡æ ‡
            if metrics.get('avg_computation_time'):
                report += f"å¹³å‡è®¡ç®—æ—¶é—´: {metrics.get('avg_computation_time', 0):.4f}s\n"
                report += f"æ€»è®¡ç®—æ—¶é—´: {metrics.get('total_computation_time', 0):.2f}s\n"
            
            report += "\n"
        
        return report
    
    def save_results(self, benchmark_results: Dict[str, Any], output_dir: str = "results"):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        import json
        import os
        from datetime import datetime
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = os.path.join(output_dir, f"benchmark_results_{timestamp}.json")
        
        # æ¸…ç†ç»“æœä»¥ä¾¿JSONåºåˆ—åŒ–
        clean_results = {}
        for algo_name, data in benchmark_results.items():
            clean_results[algo_name] = {
                'metrics': data['metrics'],
                'error': data.get('error', None)
            }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜å¯¹æ¯”è¡¨æ ¼
        table_file = os.path.join(output_dir, f"benchmark_table_{timestamp}.txt")
        with open(table_file, 'w', encoding='utf-8') as f:
            f.write(self.generate_comparison_table(benchmark_results))
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = os.path.join(output_dir, f"benchmark_report_{timestamp}.txt")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(self.generate_detailed_report(benchmark_results))
        
        print(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜:")
        print(f"   è¯¦ç»†ç»“æœ: {results_file}")
        print(f"   å¯¹æ¯”è¡¨æ ¼: {table_file}")
        print(f"   è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        return results_file, table_file, report_file


def run_quick_benchmark(topology: NetworkTopology, traffic_demands: List[TrafficDemand],
                       algorithms: List[str] = None) -> Dict[str, Any]:
    """å¿«é€ŸåŸºå‡†æµ‹è¯•çš„ä¾¿æ·å‡½æ•°"""
    manager = BenchmarkManager()
    results = manager.run_benchmark(topology, traffic_demands, algorithms)
    
    # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
    print("\n" + manager.generate_comparison_table(results))
    
    return results
